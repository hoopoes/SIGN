{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## SIGN: Scalable Inception Graph Networks  \r\n",
    "- experiments with flickr dataset  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.nn import Linear\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "from torch_geometric.datasets import Flickr\r\n",
    "import torch_geometric.transforms as T\r\n",
    "\r\n",
    "from torch_sparse import SparseTensor\r\n",
    "\r\n",
    "import pytorch_lightning as pl\r\n",
    "\r\n",
    "sys.path.append('../')\r\n",
    "from utils import *\r\n",
    "from torch_custom_funcs import *\r\n",
    "logger = make_logger(name='sign_logger')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Data 변화 체크하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Preparing Data\r\n",
    "K = 4\r\n",
    "\r\n",
    "path = os.path.join(os.getcwd(), 'data', 'Flickr')\r\n",
    "transform = T.Compose([T.NormalizeFeatures(), T.SIGN(K)])\r\n",
    "dataset = Flickr(path, transform=T.NormalizeFeatures())\r\n",
    "data = dataset[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Check\r\n",
    "inspector = GraphInspector(data)\r\n",
    "inspector.get_basic_info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'num_nodes': 89250, 'num_edges': 899756}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "inspector.inspect('edge_index')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "most_freq_appeared_node: 50 with 5425\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adjacency Matrix의 변화에 대해 확인한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "row, col = data.edge_index\r\n",
    "adj_t = SparseTensor(\r\n",
    "    row=col, col=row, sparse_sizes=(data.num_nodes, data.num_nodes))\r\n",
    "\r\n",
    "deg = adj_t.sum(dim=1).to(torch.float)\r\n",
    "deg_inv_sqrt = deg.pow(-0.5)\r\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\r\n",
    "adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\r\n",
    "\r\n",
    "a1 = adj_t\r\n",
    "a2 = a1 @ a1\r\n",
    "a3 = a2 @ a1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(\"Density Change: {:.4f}, {:.4f}, {:.4f}\".format(a1.density(), a2.density(), a3.density()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Density Change: 0.0001, 0.0097, 0.1114\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$AX$ 의 변화에 대해 확인한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Ax\r\n",
    "x1 = adj_t @ data.x\r\n",
    "x2 = adj_t @ x1\r\n",
    "x3 = adj_t @ x2\r\n",
    "x4 = adj_t @ x3\r\n",
    "\r\n",
    "# Cosine Sim\r\n",
    "x_list = [x1, x2, x3, x4]\r\n",
    "\r\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-8)\r\n",
    "N = 4\r\n",
    "output = np.zeros((N, N))\r\n",
    "for i in range(N):\r\n",
    "    for j in range(N):\r\n",
    "        output[i, j] = cos(x_list[i], x_list[j]).detach().mean().item()\r\n",
    "\r\n",
    "print(pd.DataFrame(output))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          0         1         2         3\n",
      "0  1.000000  0.806187  0.883690  0.824013\n",
      "1  0.806187  1.000000  0.961076  0.984005\n",
      "2  0.883690  0.961076  1.000000  0.985965\n",
      "3  0.824013  0.984005  0.985965  1.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2단계를 건널 경우 유사도가 높아지는 것으로 보인다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Wrapping with DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "K = 4\r\n",
    "BATCH_SIZE = 1024\r\n",
    "\r\n",
    "path = os.path.join(os.getcwd(), 'data', 'Flickr')\r\n",
    "transform = T.Compose([T.NormalizeFeatures(), T.SIGN(K)])\r\n",
    "dataset = Flickr(path, transform=transform)\r\n",
    "data = dataset[0]\r\n",
    "\r\n",
    "device = get_device()\r\n",
    "\r\n",
    "train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\r\n",
    "val_idx = data.val_mask.nonzero(as_tuple=False).view(-1)\r\n",
    "test_idx = data.test_mask.nonzero(as_tuple=False).view(-1)\r\n",
    "\r\n",
    "train_loader = DataLoader(train_idx, batch_size=BATCH_SIZE, shuffle=True)\r\n",
    "val_loader = DataLoader(val_idx, batch_size=BATCH_SIZE)\r\n",
    "test_loader = DataLoader(test_idx, batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Model & Lightning Module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class GNN(torch.nn.Module):\r\n",
    "    def __init__(self, hidden_size, drop_rate):\r\n",
    "        super(GNN, self).__init__()\r\n",
    "\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.drop_rate = drop_rate\r\n",
    "\r\n",
    "        # MLP for downstream tasks\r\n",
    "        self.lins = torch.nn.ModuleList()\r\n",
    "        self.bns = torch.nn.ModuleList()\r\n",
    "\r\n",
    "        for _ in range(K+1):\r\n",
    "            self.lins.append(Linear(dataset.num_node_features, hidden_size))\r\n",
    "            self.bns.append(nn.BatchNorm1d(num_features=hidden_size))\r\n",
    "\r\n",
    "        self.linear_final = Linear((K+1)*hidden_size, dataset.num_classes)\r\n",
    "\r\n",
    "        self.reset_parameters()\r\n",
    "\r\n",
    "    def reset_parameters(self):\r\n",
    "        for lin in self.lins:\r\n",
    "            lin.reset_parameters()\r\n",
    "        for bn in self.bns:\r\n",
    "            bn.reset_parameters()\r\n",
    "\r\n",
    "    def forward(self, xs):\r\n",
    "        hs = []\r\n",
    "        for i, x in enumerate(xs):\r\n",
    "            h = self.lins[i](x)\r\n",
    "            h = self.bns[i](h)\r\n",
    "            h = F.relu(h)\r\n",
    "            h = F.dropout(h, p=self.drop_rate, training=self.training)\r\n",
    "            hs.append(h)\r\n",
    "        h = torch.cat(hs, dim=-1)\r\n",
    "        h = self.linear_final(h)\r\n",
    "        return h.log_softmax(dim=-1)\r\n",
    "\r\n",
    "\r\n",
    "class TorchGraph(pl.LightningModule):\r\n",
    "    def __init__(self, device):\r\n",
    "        super(TorchGraph, self).__init__()\r\n",
    "        self.model = GNN(hidden_size=HIDDEN_SIZE, drop_rate=DROP_RATE).to(device)\r\n",
    "\r\n",
    "    def forward(self, idx):\r\n",
    "        self.model.eval()\r\n",
    "\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        # batch = idx\r\n",
    "        self.model.train()\r\n",
    "\r\n",
    "        xs = [data.x[batch].to(device)]\r\n",
    "        xs += [data[f'x{i}'][batch].to(device) for i in range(1, K+1)]\r\n",
    "        y_true = data.y[batch].to(device)\r\n",
    "\r\n",
    "        y_pred = self.model(xs)\r\n",
    "        loss = F.nll_loss(y_pred, y_true)\r\n",
    "\r\n",
    "        self.log(name=\"train_loss\", value=loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        self.model.eval()\r\n",
    "\r\n",
    "        xs = [data.x[batch].to(device)]\r\n",
    "        xs += [data[f'x{i}'][batch].to(device) for i in range(1, K+1)]\r\n",
    "        y_true = data.y[batch].to(device)\r\n",
    "\r\n",
    "        y_pred = self.model(xs)\r\n",
    "        loss = F.nll_loss(y_pred, y_true)\r\n",
    "\r\n",
    "        self.log(name=\"val_loss\", value=loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def configure_optimizers(self):\r\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=LR)\r\n",
    "        return optimizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "HIDDEN_SIZE = 1024\r\n",
    "DROP_RATE = 0.5\r\n",
    "LR = 0.01\r\n",
    "EPOCHS = 200\r\n",
    "\r\n",
    "sign_graph = TorchGraph(device=device)\r\n",
    "logger.info(f\"num model parameters: {get_num_params(sign_graph.model)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-08 20:29:11,375 - sign_logger - num model parameters: 2611207\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(sign_graph.model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GNN(\n",
      "  (lins): ModuleList(\n",
      "    (0): Linear(in_features=500, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=1024, bias=True)\n",
      "    (2): Linear(in_features=500, out_features=1024, bias=True)\n",
      "    (3): Linear(in_features=500, out_features=1024, bias=True)\n",
      "    (4): Linear(in_features=500, out_features=1024, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear_final): Linear(in_features=5120, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "param_report = get_parameter_report(sign_graph.model)\r\n",
    "print(param_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                   name        shape  num_param\n",
      "0         lins.0.weight  [1024, 500]     512000\n",
      "1           lins.0.bias       [1024]       1024\n",
      "2         lins.1.weight  [1024, 500]     512000\n",
      "3           lins.1.bias       [1024]       1024\n",
      "4         lins.2.weight  [1024, 500]     512000\n",
      "5           lins.2.bias       [1024]       1024\n",
      "6         lins.3.weight  [1024, 500]     512000\n",
      "7           lins.3.bias       [1024]       1024\n",
      "8         lins.4.weight  [1024, 500]     512000\n",
      "9           lins.4.bias       [1024]       1024\n",
      "10         bns.0.weight       [1024]       1024\n",
      "11           bns.0.bias       [1024]       1024\n",
      "12         bns.1.weight       [1024]       1024\n",
      "13           bns.1.bias       [1024]       1024\n",
      "14         bns.2.weight       [1024]       1024\n",
      "15           bns.2.bias       [1024]       1024\n",
      "16         bns.3.weight       [1024]       1024\n",
      "17           bns.3.bias       [1024]       1024\n",
      "18         bns.4.weight       [1024]       1024\n",
      "19           bns.4.bias       [1024]       1024\n",
      "20  linear_final.weight    [7, 5120]      35840\n",
      "21    linear_final.bias          [7]          7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "trainer = pl.Trainer(\r\n",
    "    gpus=1,\r\n",
    "    auto_scale_batch_size=None,\r\n",
    "    deterministic=True,\r\n",
    "    max_epochs=EPOCHS\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "trainer.fit(model=sign_graph, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\Youyoung\\Documents\\SIGN\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:530: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GNN  | 2.6 M \n",
      "-------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.445    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\Youyoung\\Documents\\SIGN\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                                      "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\Youyoung\\Documents\\SIGN\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Youyoung\\Documents\\SIGN\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:322: UserWarning: The number of training samples (44) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 199: 100%|██████████| 66/66 [00:01<00:00, 66.60it/s, loss=0.082, v_num=0, train_loss_step=0.0843, val_loss_step=6.270, val_loss_epoch=6.320, train_loss_epoch=0.0845]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training Loss가 빠르게 줄어드는 것에 비하여 Validation Loss는 오히려 발산하고 있다. 오직 Training Data 자체에 대해서만 특징을 학습하고 있는 것으로 추측된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Evaluate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "@torch.no_grad()\r\n",
    "def test(model, loader):\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    total_correct = total_examples = 0\r\n",
    "    for idx in loader:\r\n",
    "        xs = [data.x[idx].to(device)]\r\n",
    "        xs += [data[f'x{i}'][idx].to(device) for i in range(1, K + 1)]\r\n",
    "        y = data.y[idx].to(device)\r\n",
    "\r\n",
    "        out = model(xs)\r\n",
    "        total_correct += int((out.argmax(dim=-1) == y).sum())\r\n",
    "        total_examples += idx.numel()\r\n",
    "\r\n",
    "    return total_correct / total_examples\r\n",
    "\r\n",
    "train_acc = test(sign_graph.model.to(device), train_loader)\r\n",
    "val_acc = test(sign_graph.model.to(device), val_loader)\r\n",
    "test_acc = test(sign_graph.model.to(device), test_loader)\r\n",
    "\r\n",
    "print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train: 0.9758, Val: 0.4814, Test: 0.4837\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Overfitting이 심하여 Generalization이 잘 되지 않는 결과를 보여준다. 논문의 실험 결과에서는 F1 Score가 Train/Val/Test 중 어떤 데이터셋에서 나온 결과물인지 명확히 밝히고 있지 않다.  "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "eb94d6aba11e4b9a914bc9f6af958969303027bf976984fb080250ca9121c6b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}